{"componentChunkName":"component---src-templates-notes-js","path":"/operating-system/memory-management","result":{"data":{"markdownRemark":{"html":"<h1>Virtual Memory</h1>\n<p>page, page fault, address translation, main memory, storage.</p>\n<p>Divide virtual memory into fix-sized blocks, called <strong>page</strong>. Each page has $2^p$ bytes. similarly, physical memory are also divided into same-sized blocks, called <strong>page frame</strong>.</p>\n<p>virtual page status:</p>\n<ul>\n<li>not allocated: not allocated / created pages. Unallocated pages have no associated data.</li>\n<li>cached: allocated and have associated page frame.</li>\n<li>not cached: not cached in physical memory</li>\n</ul>\n<h2>Memory Abstraction</h2>\n<h3>No Abstraction</h3>\n<p>First let's answer the questions:</p>\n<ul>\n<li>Why do we need abstraction? What would happen if we have no memory abstraction?</li>\n</ul>\n<p>The simplest memory abstraction is no abstraction. The program instruction specifies the physical address directly.</p>\n<p>Under this condition, running multiple programs at the same time becomes a challenge. Because the physical memory addresses are used directly, one program can end up overwrite content of another program while executing.</p>\n<p>It is still possible to run multiple programs with <strong>swapping</strong>: let the OS copy the entire contents of memory to a disk file and run the next program. IBM 360 provided hardware solution, by using a protective key on memory blocks.</p>\n<p>And even if the program can run at the same time, there is one more challenge. The physical addresses in program instructions are constant, but programs can be dynamically loaded and the starting address can be random. This renders the constant physical address useless. One solution is to use <strong>static relocation</strong> -- the program is modified during loading. The constant addresses are modified with offset. However, it's slow and complicated. The loader needs extra information on which addresses need to be modified.</p>\n<h3>Address Space</h3>\n<p>Each and every byte in the address space have a unique address.</p>\n<h4>Base and limit register</h4>\n<h3>Swapping</h3>\n<h4>Memory compaction</h4>\n<p>merge small holes into bigger chunks of free memory</p>\n<h4>Memory management with bitmaps</h4>\n<h4>Memory management with linked list</h4>\n<p>allocation algorithms</p>\n<ul>\n<li>first fit</li>\n<li>next fit</li>\n<li>best fit</li>\n<li>worst fit</li>\n<li>quick fit: separate lists for common requests</li>\n</ul>\n<h2>Virtual Memory</h2>\n<p>The basic idea behind virtual memory is that each program has its own address space, which is broken up into chunks called pages. Each page is a contiguous range of addresses. These pages are mapped onto physical memory, but not all pages have to be in physical memory at the same time to run the program. When the program references a part of its address space that is in physical memory, the hardware performs the necessary mapping on the fly. When the program references a part of its address space that is not in physical memory, the operating system is alerted to go get the missing piece and re-execute the instruction that failed.</p>\n<p>virtual addresses are mapped by a special piece of hardware called the <strong>Memory Management Unit (MMU)</strong>. An MMU maps virtual addresses onto physical memory addresses.</p>\n<h3>Paging</h3>\n<p>The virtual memory address space is divided into fixed-sized ($2^n$ bytes) units called <strong>pages</strong>, the corresponding units in the physical memory are called <strong>page frames</strong>. The page and page frames generally have the same size.</p>\n<h4>Page Table</h4>\n<p>The Page table stores mapping from virtual pages to page frames. The page table is used by MMU for the address translation. The operating system is responsible for maintaining the page table.</p>\n<h4>Page table entry</h4>\n<ul>\n<li><strong>page frame number</strong></li>\n<li><strong>present bit (valid bit)</strong>: access page with this bit set to 0 will cause a page fault.</li>\n<li><strong>protection bit</strong>: specify what kind of access are permitted. With 3 bits we can show if reading, writing or execution is allowed.</li>\n<li><strong>modified bit (dirty bit)</strong>: When a page is written to, the modified bit is set. when page is reclaimed, dirty pages must be written back to disk.</li>\n<li><strong>referenced bit</strong>: set when referenced, no matter read or write</li>\n<li><strong>caching disabled bit</strong>: important for pages mapped onto device registers (for memory-mapped I/O).</li>\n</ul>\n<h3>Speed up paging</h3>\n<p>There are 2 major performance consideration that the virtual memory design should address</p>\n<ol>\n<li>Address translation must be fast.</li>\n<li>\n<p>If virtual memory space is large, the page table will be large (cost memory).</p>\n<ul>\n<li>e.g. with 4KB sized pages, a 32-bit address space (4GB) has 1,000,000 pages. If each entry cost 4 bytes, the page table can grow to 4MB. 64-bit address spaces will have much more.</li>\n</ul>\n</li>\n</ol>\n<h4>Translation Look-aside Buffer (TLB)</h4>\n<p>TLB is a cache to speed up address translation by avoiding accessing the page table each time. TLB is usually inside the MMU and consists of a small number of entries (rarely more than 256). Each entry consists of information about one page, including virtual page number, page frame number, protection/modified bit, etc.</p>\n<p>Note some system does not have dedicated hardware support, instead they use a software TLB approach.</p>\n<h4>Multilevel Page Table</h4>\n<p>The key idea of multilevel page table is to <strong>avoid keeping all the page tables in memory at all times</strong>. By using multiple smaller tables, only some of the tables need to be in memory.</p>\n<p>For example for a 32-bit address space with 2-level page table and 4KB page size, we can divide the 32-bit into <code class=\"language-text\">10-bit (PT1) + 10-bit (PT2) + 12-bit (offset 4kb)</code>.</p>\n<ul>\n<li>The MMU first use <code class=\"language-text\">PT1</code> to lookup the top-level page table and obtain a entry, pointing to a second-level page table representing a 4MB (<code class=\"language-text\">10-bit + 12-bit</code>) address space.</li>\n<li>The MMU then use <code class=\"language-text\">PT2</code> to lookup the second-level page table and find the corresponding entry with the page frame number.</li>\n<li>MMU computes address from page frame number + offset and send the address via bus to memory.</li>\n</ul>\n<p>This process shows that although the address contains over a million page, only 2 tables is used (1 top-level + 1 second-level).</p>\n<h4>Address translation</h4>\n<p>Success translation flow:</p>\n<ol>\n<li>CPU take the virtual memory address, and pass it to MMU.</li>\n<li>MMU generate the address of the page table entry, and request the entry from TLB. If TLB miss, try cpu cache/main memory.</li>\n<li>MMU gets the entry data.</li>\n<li>MMU construct the physical address from page table entry and virtual address, and request data from cache/main memory.</li>\n<li>cache/main memory pass the requested data to CPU.</li>\n</ol>\n<p>Page fault causing page replacement flow:</p>\n<ol>\n<li>Same as above</li>\n<li>Same as above</li>\n<li>Same as above</li>\n<li>MMU get the PTE but the valid bit is zero, cause page fault. Trap into kernel for exception handling.</li>\n<li>Exception handling program find the page frame to replace, if it is modified, write back to disk first.</li>\n<li>Exception handling program replace the page and update page table entry in memory.</li>\n<li>Exception handling program return control to original process, retry the instruction that caused page fault. (back to step 1.)</li>\n</ol>\n<p>Note: this is simplified and not specific to particular OS.</p>\n<h2>Page Replacement Algorithm</h2>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Comment</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Optimal</td>\n<td>Not implementable, but useful as a benchmark</td>\n</tr>\n<tr>\n<td>FIFO (First-In, First-Out)</td>\n<td>Might throw out important pages</td>\n</tr>\n<tr>\n<td>Second chance</td>\n<td>Big improvement over FIFO</td>\n</tr>\n<tr>\n<td>Clock</td>\n<td>Realistic implementation of FIFO/Second chance</td>\n</tr>\n<tr>\n<td>LRU (Least Recently Used)</td>\n<td>Excellent, but difficult to implement exactly</td>\n</tr>\n<tr>\n<td>NRU (Not Recently Used)</td>\n<td>Very crude approximation of LRU</td>\n</tr>\n<tr>\n<td>NFU (Not Frequently Used)</td>\n<td>Fairly crude approximation to LRU</td>\n</tr>\n<tr>\n<td>Aging</td>\n<td>Efficient algorithm that approximates LRU well</td>\n</tr>\n<tr>\n<td>Working set</td>\n<td>Somewhat expensive to implement</td>\n</tr>\n<tr>\n<td>WSClock</td>\n<td>Good efficient algorithm</td>\n</tr>\n</tbody>\n</table>\n<h2>Segmentation</h2>\n<p>Multiple independent logical address space called <strong>segments</strong>. Different segments may and often will have different lengths. Moreover, segments lengths may change during execution (think of a stack that may grow or shrink). As segments can have different size, replacement can introduce holes in between segments. This is called <strong>checkerboarding</strong> or <strong>external fragmentation</strong>, wasting memory in holes. It can be dealt with <strong>compaction</strong>.</p>\n<blockquote>\n<p>Note that segments are LOGICAL units, physically could still be implemented with paging. Each segment is linear, but multiple segments together forms a 2-dimensional memory space. To address a memory location, you need to specify the segment number as well as the address within the segment.</p>\n</blockquote>\n<h3>Paging vs Segmentation</h3>\n<table>\n<thead>\n<tr>\n<th>Paging</th>\n<th>Segmentation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>physical</td>\n<td>logical</td>\n</tr>\n<tr>\n<td>1 linear address space</td>\n<td>2 dimensional address space consists of multiple segments. Each segment is linear.</td>\n</tr>\n<tr>\n<td>a page is fix-sized</td>\n<td>segment size is variable</td>\n</tr>\n</tbody>\n</table>\n<h3>Segmentation with paging: MULTICS</h3>\n<p>Each address consists of the segment number and the address within the segment. The address within the segment is further divided into a page number and a word within the page. Each segment has its own page table. Every segment number associates with a segment descriptor. The segment descriptor contains the main memory address of the page table, segment length, and other bits.</p>\n<p>A memory reference works like below:</p>\n<ol>\n<li>The segment number was used to find the segment descriptor.</li>\n<li>A check was made to see if the segmentâ€™s page table was in memory. If it was, it was located. If it was not, a segment fault occurred. If there was a protection violation, a fault (trap) occurred.</li>\n<li>The page table entry for the requested virtual page was examined. If the page itself was not in memory, a page fault was triggered. If it was in memory, the main-memory address of the start of the page was extracted from the page table entry.</li>\n<li>The offset was added to the page origin to give the main memory address where the word was located.</li>\n<li>The read or store finally took place.</li>\n</ol>\n<h3>Segmentation with paging: x86</h3>\n<p>Up until the x86-64, the virtual memory system of the x86 resembled that of MULTICS in many ways, including the presence of both segmentation and paging.</p>\n<blockquote>\n<p>The x86-64 architecture does not use segmentation in long mode (64-bit mode). It relies only on paging.</p>\n</blockquote>","frontmatter":{"title":"Memory Management"},"parent":{"__typename":"File","modifiedTime":"2021-10-26T14:41:02.023Z"}}},"pageContext":{}}}